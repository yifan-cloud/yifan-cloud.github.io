<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Yifan Xu</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-154209474-1"></script>
		<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-154209474-1');
		</script>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/yifan.jpg" alt="" /></span>
					<h4 id="logo"><a href="#">Yifan Xu</a></h4>
					<p>Master of Science, Robotics<br>University of Michigan</p>
				</header>
				<nav id="nav">
					<ul>
						<li><a href="#about" class="active">About</a></li>
						<li><a href="#research">Research</a></li>
						<li><a href="#publications">Publications</a></li>
						<!-- <li><a href="#projects">Projects</a></li> -->
						<!-- <li><a href="#teaching">Teaching</a></li>
						<li><a href="#outreach">Outreach</a></li> -->
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<li><a href="www.linkedin.com/in/yifan-xu-43876120b" class="icon brands fa-linkedin" target="_blank"><span class="label">Linkedin</span></a></li>
						<li><a href="https://github.com/yifan-cloud" class="icon brands fa-github" target="_blank"><span class="label">Github</span></a></li>
						<li><a href="https://scholar.google.com/citations?user=RYKMFp4AAAAJ&hl=en&authuser=1" class="icon solid fa-graduation-cap" target="_blank"><span class="label">Scholar</span></a></li>
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="about">
								<!-- <div class="image main" data-position="center">
									<img src="images/banner_cool.jpg" alt="" />
								</div> -->
								<div class="container">
									<header class="major">
										<h2>About</h2>
									</header>
									<p>
										I am currently a second year Master student in <a href="https://robotics.umich.edu/" target="_blank">Robotics</a> at the University of Michigan.
										My research advisor is <a href="https://robotics.umich.edu/profile/maani-ghaffari/" target="_blank">Prof. Maani Ghaffari</a> 
										and I am a research assistant and team leader of the <a href="https://curly.engin.umich.edu/" target="_blank">Computational Autonomy and Robotics Laboratory (CURLY)</a>. <br><br>
										I hold a B.S. degree in Robotics from Northeastern University(China).

										My research interests lies in SLAM, path planning and reinforcement learning. 
										<!-- I am a second-year Master's student in the <a href="https://robotics.umich.edu/" target="_blank">Robotics Institute</a> 
										at the <a href="https://umich.edu/" target="_blank">University of Michigan</a>. 
										I'm currently working in <a href="https://www.biped.solutions/" target="_blank">Biped Robotics Lab</a> and <a href="http://robots.engin.umich.edu/" target="_blank">the Perceptual Robotics Laboratory (PeRL)</a>,
										advised by <a href="http://ece.umich.edu/faculty/grizzle/" target="_blank">Prof. Jessy Grizzle</a>, <a href="http://robots.engin.umich.edu/~ryan/" target="_blank">Prof. Ryan Eustice</a>, and <a href="https://www.maanighaffari.com/" target="_blank">Dr. Maani Ghaffari</a>.
										I hold a B.S. degree in Mechanical Engineering from National Taiwan University.

										My research interest falls in perception and reasoning in robotics, including 3D geometric and semantic understanding, localization and mapping, and motion planning.</p> -->
																		
									<p>
										Email: yfx [at] umich [dot] edu					
									</p>

									<!-- <p><a href="CV_Justin.pdf" target="_blank">[CV]</a><a href="https://scholar.google.com/citations?user=1HY3TXcAAAAJ&hl=en&authuser=1" target="_blank">[Google Scholar]</a></p> -->
								
									<!-- <h3>News</h3>
									<div class="features">
										<article>
											November 16, 2021 <br>
											<p_news> I will be the Graduate Student Instructor of ROB530: Mobile Robotics for Winter 2022!</p_news>
										</article>
										<article>
											September 28, 2021 <br>
											<p_news> Check out our latest Mini Cheetah video.</p_news> <br>
											<iframe width="560" height="315" src="https://www.youtube.com/embed/oVbP-Y8xT_E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
										</article>
										<article>
											September 13, 2021 <br>
											<p_news> Our paper "Legged Robot State Estimation using Invariant Kalman Filtering and Learned Contact Events" is accepted to 2021 Conference on Robot Learning (CoRL)!</p_news>
										</article>
										<article>
											May 30, 2021 <br>
											<p_news> Our paper "A New Framework for Registration of Semantic Point Clouds from Stereo and RGB-D Cameras" is published in 2021 IEEE International Conference on Robotics and Automation (ICRA)!</p_news>
										</article>
									</div> -->
							</div>
							</section>

						<!-- Two -->
							<section id="research">
								<div class="container">
									<h3>Research</h3>
									<p>
									</p>
									<div class="features">
											<article>
												<!-- <iframe width="340" height="200" src="https://www.youtube.com/embed/oVbP-Y8xT_E" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
										
												<a href="images/PCR.png" class="image"><img src="images/PCR.png" alt="" /></a>
												<div class="inner">
													<h4>Providers-Clients-Robots</h4>
													<p>
														This work proposes a novel architecture recognizing three interacting parties, Providers (people in-charge at an establishment), Clients (visitors of the establishment), and Robots (PCR) that are involved in a socially assistive setting. We present an instance of a spatial interaction pipeline within a PCR framework for applications in socially assistive robots detailing a mapping and planning pipeline with the context of a robot in indoor spaces such as an office building, museum, art gallery, or shopping mall. The proposed pipeline successfully uses a shared understanding of the environment for improved and interactive planning. Within the PCR framework, we solve the planning problem as one of understanding and encoding preferences from the Providers and the Clients into planning Robot behavior. 

														<br>
														
														<a href="https://openreview.net/forum?id=yt3tDB67lc5" target="_blank">[Paper]</a><br>
														<!-- <a href="https://github.com/UMich-CURLY/deep-contact-estimator" target="_blank">[Deep-Contact-Estimator]</a><br> -->
														<a href="https://github.com/UMich-CURLY/spatial_interaction" target="_blank">[Code]</a>
														
													</p>
													
												</div>
												<!-- <a href="images/cvo_traj.png" class="image"><img src="images/cvo_traj.png" alt="" /></a> -->
											</article>
											<article>
												<a href="images/IRL.png" class="image"><img src="images/IRL.png" alt="" /></a>
												
												<div class="inner">
													<h4>Inverse Reinforcement Learning based Local Planner Design in Crowded Environment</h4>
													<p>
														We contructed a novel feature layer combination used in Inverse Reinforcement Learning(IRL) to handle navigation problem in complicated and crowded dynamic environment. We seek to find a proposed local planner can not only help the robot navigate through crowded environment safely, but also follow some social norms that the robot would not affect normal people. Besides the feature map contruction, we also proposed a novel way to calculate the trajectory ranking loss term. This is added to the Max Entropy gradient calculation process to decrease the bad effect of non-optimal or suboptimal demonstration when applying the IRL algorithm. 
														<a href="https://github.com/UMich-CURLY/Fetch_IRL" target="_blank">[Code]</a>
													</p>
													
												</div>
										</div>
								</div>
							</section>

						<!-- Three -->
							<section id="publications">
								<div class="container">
									<h3>Publications</h3>
									<h4>Conference Papers</h4>
									<ul>
										<p1><li>Tribhi Kathuria, <b>Yifan Xu*</b>, Theodor Chakhachiro*, X. Jessie Yang, and Maani Ghaffari <b>Providers-Clients-Robots: Framework for spatial-semantic planning for shared understanding in human-robot interaction</b> In 31st IEEE International Conference on Robot & Human Interactive Communication, 2022 &nbsp; <a href="https://arxiv.org/abs/2206.10767" target="_blank">[Paper]</a><a href="https://github.com/UMich-CURLY/spatial_interaction" target="_blank">[Code]</a></li></p1>
										
									</ul>
									
								</div>
							</section>

						<!-- Four -->
							<!-- <section id="projects">
								<div class="container">
									<h3>Projects</h3>
									<div class="features">
										<article>
											<a href="images/image_caption_generator.png" class="image"><img src="images/image_caption_generator.png" alt="" /></a>
											<div class="inner">
												<h4>Image Caption Generator with Simple Semantic Segmentation</h4>
												<p>
													Utilized a pre-trained ImageNet as the encoder, and a Long-Short Term Memory (LSTM) net with attention module as the decoder in PyTorch that can automatically generate properly formed English sentences of the inputted images. 
													Implemented a simple semantic segmentation algorithm by highlighting the attention layer used to generate the English sentence.
												</p>
											</div>
										</article>
										<article>
											<a href="images/DVO_with_BA.JPG" class="image"><img src="images/DVO_with_BA.JPG" alt="" /></a>
											<div class="inner">
												<h4>Direct Visual Odometry with Pose Graph Optimization and LoopClosure</h4>
												<p>
													Implemented an offline direct visual odometry algorithm with pose-graph optimization to track a robot’s position using RGB-D camera image as input.
												</p>
											</div>
										</article>
										<article>
												<a href="images/botlab.png"  class="image"><img src="images/botlab.png" alt="" /></a>
												<div class="inner">
													<h4>Simultaneous Localization and Mapping (SLAM) Robot with Particle Filter and Path Planning</h4>
													<p>
														Implemented a particle filter based simultaneous localization and mapping (SLAM) system and A* path planning algorithm
														for a robot with 2D LiDAR to explore and escape an arbitrarily-configured maze.
													</p>
												</div>
											</article>
										<article>
											<a href="images/project-selfdriving.png"  class="image"><img src="images/project-selfdriving.png" alt="" /></a>
											<div class="inner">
												<h4>Vehicle Classification and Localization</h4>
												<p>Utilized OpenCV for image preprocessing and TensorFlow for DenseNet deep learning to classify 22 different types of vehicle from a video game and localize them by point cloud data.</p>
											</div>
										</article>
										<article>
											<h4>6 DOF Robot Arm with a 3D Block Detector and Color Segmentation</h4>
											<iframe width="560" height="315" src="https://www.youtube.com/embed/vHC6HfZUM6k" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
											
											<p>
												We developed a working system capable of autonomously identifying and manipulating colored blocks with a 3-link robotic manipulator. 
												The tasks we accomplished with the robot arm includes: Stacked the blocks in a specific color order, moved and mirrored the position of the blocks in one side of the plane to the other side, and built a pyramid with the wooden blocks.
											</p>
											
										</article>
									</div>
									<p>

									</p>
								</div>
							</section> -->
				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>